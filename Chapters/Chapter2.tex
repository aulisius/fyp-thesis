% Chapter 2
\chapter{RELATED WORK} % Chapter Title in ALL CAPSacs
Loops are a common target of various compiler optimization techniques. LLVM itself by default supports upto 10 optimizations targeted at loops. But there still is a lot left to be done especially in the area of reducing data accesses within the loops.

In \cite{oh2013practical}, Taewook Oh et all discusses exploiting predictable patterns of values across loop iterations, As existing specializers cannot fully capitalize on this opportunity. To address this limitation, They have presented Invariant-induced Pattern based Loop Specialization (IPLS), the first fully-automatic specialization technique. Using dynamic information-flow tracking, IPLS profiles the values of instructions that depend solely on invariants and recognizes repeating patterns across multiple iterations of hot loops. IPLS then specializes these loops, using those patterns to predict values across a large window of loop iterations. This enables aggressive optimization of the loop.

Another style of optimization is with regard to invariants in a loop. LLVM includes ‘loop invariant code motion’ but \cite{sharma2013data} focuses on algebraic invariants found through a data-driven approach. The task of generating loop invariants lies at the heart of any program verification technique. A wide variety of techniques have been developed for generating linear invariants, including methods based on abstract interpretation and constraint solving among others. Recently, researchers have also applied these techniques to the generation of non-linear loop invariants. These techniques discover algebraic invariants.


Current parallelisation techniques instruction level parallelisation well in a satisfactory manner, but when it comes to paralellsiation across of loop iterations, it’s not exploited well, in \cite{aiken1988optimal}, Alexander Aiken and Alexandru Nicolau  present a new technique bridging the gap between fine- and coarse-grain loop parallelization, allowing the exploitation of parallelism inside and across loop iterations. Furthermore, we show that, given a loop and a set of dependencies between its statements, the execution schedule obtained by out transformation is time optimal: no transformation of the loop based on the given data-dependencies can yield a shorter running time for that loop.


In \cite{wolf1991data}, Wolf, Michael E., and Monica S. Lam discuss about improving the data locality. It is achieved by loop transformation algorithm which is based on two concepts: a mathematical formulation of reuse and locality, and a loop transformation theory that unifies the various transforms as unimodular matrix transformations. The algorithm is successful in optimizing codes such as matrix multiplication, successive over-relaxation (SOR), LU decomposition without pivoting, and Givens QR factorization. Performance evaluation indicates that locality optimization is especially crucial for scaling up the performance of parallel code.


In \cite{amato2012discovering}, G. Amato, M. Parton, and F. Scozzari propose a new technique combining dynamic and static analysis of programs to find linear invariants. We use a statistical tool, called simple component analysis, to analyze partial execution traces of a given program. They get a new coordinate system in the vector space of program variables, which is used to specialize numerical abstract domains.

Most conventional compilers fail to allocate array elements to registers because standard data-ow analysis treats arrays like scalars, making it impossible to analyze the definitions and uses of individual array elements. This deficiency is particularly troublesome for floating-point registers, which are most often used as temporary repositories for subscripted variables. In \cite{callahan1990improving}, D. Callahan, S, Carr, and K. Kennedy present a source-to-source transformation, called scalar replacement, that finds opportunities for reuse of subscripted variables and replaces the references involved by references to temporary scalar variables. The objective is to increase the likelihood that these elements will be assigned to registers by the coloring-based register allocators found in most compilers. In addition, They present transformations to improve the overall effectiveness of scalar replacement and show how these transformations can be applied in a variety of loop nest types.

In \cite{wolf1991loop} M. E. Wolf and M. S. Lam, proposes a new approach to transformations for general loop nests. In this approach, They unify all combinations of loop interchange, skewing and reversal as unimodular transformations. The use of matrices to model transformations has previously been applied only to those loop nests whose dependences can be summarized by distance vectors. Their technique is applicable to general loop nests where the dependences include both distances and directions.

Predictive commoning is a rather new approach applied in optimising loops by combining and modifying two earlier known methods namely, ’Common Subexpression Elimination’ and ’Loop-Invariant Code Motion’. It involves predicting whether array accesses will be reused in subsequent iterations and then tries to optimize those accesses by promoting them to registers.